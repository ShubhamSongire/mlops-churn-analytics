{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48918d88",
   "metadata": {},
   "source": [
    "# 03 – Raw Data Storage\n",
    "\n",
    "This notebook implements data storage strategies including:\n",
    "\n",
    "1. **Data Compression**\n",
    "   - Efficient storage formats (Parquet, compressed CSV)\n",
    "   - Compression ratio analysis\n",
    "   - Storage cost optimization\n",
    "\n",
    "2. **Data Partitioning**\n",
    "   - Time-based partitioning\n",
    "   - Feature-based partitioning\n",
    "   - Optimization for query patterns\n",
    "\n",
    "3. **Data Catalog**\n",
    "   - Automated metadata collection\n",
    "   - Schema versioning\n",
    "   - Data lineage tracking\n",
    "\n",
    "4. **Storage Analytics**\n",
    "   - Space usage monitoring\n",
    "   - Access patterns analysis\n",
    "   - Storage optimization recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e895a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Set up basic logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb280b9a",
   "metadata": {},
   "source": [
    "# Raw Data Storage Design\n",
    "\n",
    "The raw layer stores ingested data in a structured but minimally transformed form.  To\n",
    "support efficient ingestion and downstream processing the following\n",
    "layout conventions are used:\n",
    "\n",
    "* **Source partitioning** – Data is organised first by its origin.  There\n",
    "  are separate top‑level folders for each source type, e.g. `source_csv` and\n",
    "  `source_api`.  Additional sources (e.g. database dumps, SFTP imports)\n",
    "  would appear as additional top‑level partitions.\n",
    "* **Date partitioning** – Within each source folder data is stored in\n",
    "  subdirectories labelled with the extraction date in `YYYYMMDD` format.\n",
    "  This makes it easy to discover and process data from a particular\n",
    "  ingestion run.\n",
    "* **File naming** – Files under `source_csv/<date>/` retain their original\n",
    "  names (`customers.csv`, `transactions.csv`, …).  Files under\n",
    "  `source_api/<date>/` similarly retain their source names (e.g.\n",
    "  `web_logs.jsonl`).  When ingested into the raw layer a timestamped\n",
    "  prefix `ingested_<timestamp>_` is prepended to prevent name\n",
    "  collisions across runs.\n",
    "\n",
    "Example directory tree:\n",
    "\n",
    "```\n",
    "data/\n",
    "  raw/\n",
    "    source_csv/\n",
    "      20250821/\n",
    "        customers.csv\n",
    "        transactions.csv\n",
    "    source_api/\n",
    "      20250821/\n",
    "        web_logs.jsonl\n",
    "    ingested_20250821_153000_customers.csv\n",
    "    ingested_20250821_153000_transactions.csv\n",
    "    ingested_20250821_153000_web_logs.jsonl\n",
    "  logs/\n",
    "    ingestion/\n",
    "  validated/\n",
    "    20250821/\n",
    "  prepared/\n",
    "  transformed/\n",
    "```\n",
    "\n",
    "Although this assignment uses a local filesystem, the same structure\n",
    "would apply to cloud object stores (e.g., AWS S3 or GCS) with\n",
    "appropriate bucket prefixes (e.g., `s3://company-data/raw/source_csv/…`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da3c3dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw directory contents:\n",
      "['compressed',\n",
      " 'ingested_20250821_185705_customers.csv',\n",
      " 'ingested_20250821_185705_transactions.csv',\n",
      " 'ingested_20250821_185705_web_logs.jsonl',\n",
      " 'ingested_20250821_185841_customers.csv',\n",
      " 'ingested_20250821_185841_transactions.csv',\n",
      " 'ingested_20250821_185841_web_logs.jsonl',\n",
      " 'ingested_20250824_142601_telco_train.csv',\n",
      " 'ingested_20250824_142601_transactions.csv',\n",
      " 'ingested_20250824_142601_web_logs.jsonl',\n",
      " 'ingested_20250824_142608_telco_train.csv',\n",
      " 'ingested_20250824_142608_transactions.csv',\n",
      " 'ingested_20250824_142608_web_logs.jsonl',\n",
      " 'ingested_20250824_142727_customers.csv',\n",
      " 'ingested_20250824_142727_telco_train.csv',\n",
      " 'ingested_20250824_142727_transactions.csv',\n",
      " 'ingested_20250824_142727_web_logs.jsonl',\n",
      " 'ingested_20250824_143055_customers.csv',\n",
      " 'ingested_20250824_143055_transactions.csv',\n",
      " 'ingested_20250824_143055_web_logs.jsonl',\n",
      " 'partitioned',\n",
      " 'source_api',\n",
      " 'source_csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "raw_root = os.path.join('..', 'data', 'raw')\n",
    "print('Raw directory contents:')\n",
    "pprint(os.listdir(raw_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b2e9288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_csv(input_path, output_path=None, compression_type='gzip'):\n",
    "    \"\"\"Compress CSV file and analyze compression ratio.\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = input_path + '.gz'\n",
    "    \n",
    "    original_size = os.path.getsize(input_path)\n",
    "    \n",
    "    if compression_type == 'gzip':\n",
    "        with open(input_path, 'rb') as f_in:\n",
    "            with gzip.open(output_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "    compressed_size = os.path.getsize(output_path)\n",
    "    compression_ratio = original_size / compressed_size\n",
    "    \n",
    "    return {\n",
    "        'original_size': original_size,\n",
    "        'compressed_size': compressed_size,\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'output_path': output_path\n",
    "    }\n",
    "\n",
    "def convert_to_parquet(csv_path, output_path=None):\n",
    "    \"\"\"Convert CSV to Parquet format with optimization.\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = csv_path.replace('.csv', '.parquet')\n",
    "    \n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Optimize categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        df[col] = pd.Categorical(df[col])\n",
    "    \n",
    "    # Convert to parquet with compression\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, output_path, compression='snappy')\n",
    "    \n",
    "    result = {\n",
    "        'original_size': os.path.getsize(csv_path),\n",
    "        'parquet_size': os.path.getsize(output_path),\n",
    "        'compression_ratio': os.path.getsize(csv_path) / os.path.getsize(output_path)\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6747f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partitioned_structure(df, partition_cols, base_path):\n",
    "    \"\"\"Create a partitioned data structure based on specified columns.\"\"\"\n",
    "    \n",
    "    def get_partition_path(partition_values):\n",
    "        \"\"\"Generate partition path for given partition values.\"\"\"\n",
    "        parts = []\n",
    "        if not isinstance(partition_values, tuple):\n",
    "            partition_values = (partition_values,)\n",
    "        for col, val in zip(partition_cols, partition_values):\n",
    "            # Clean partition value\n",
    "            val = str(val).replace('/', '_').replace('\\\\', '_')\n",
    "            parts.append(f\"{col}={val}\")\n",
    "        return os.path.join(*parts)\n",
    "    \n",
    "    # Ensure the base path exists\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # Group and save data\n",
    "    for partition_values, partition_df in df.groupby(partition_cols):\n",
    "        # Create partition path\n",
    "        partition_path = get_partition_path(partition_values)\n",
    "        full_path = os.path.join(base_path, partition_path)\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        \n",
    "        # Convert categorical columns to strings to avoid pyarrow extension type issues\n",
    "        for col in partition_df.select_dtypes(include=['category']):\n",
    "            partition_df[col] = partition_df[col].astype(str)\n",
    "        \n",
    "        # Save using pyarrow directly\n",
    "        table = pa.Table.from_pandas(partition_df)\n",
    "        pq.write_table(table, os.path.join(full_path, 'data.parquet'))\n",
    "    \n",
    "    return base_path\n",
    "\n",
    "def analyze_partition_distribution(base_path):\n",
    "    \"\"\"Analyze the distribution of data across partitions.\"\"\"\n",
    "    partition_sizes = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == 'data.parquet':\n",
    "                file_path = os.path.join(root, file)\n",
    "                size = os.path.getsize(file_path)\n",
    "                relative_path = os.path.relpath(root, base_path)\n",
    "                \n",
    "                # Read parquet file using pyarrow directly\n",
    "                parquet_file = pq.ParquetFile(file_path)\n",
    "                num_rows = parquet_file.metadata.num_rows\n",
    "                \n",
    "                partition_sizes.append({\n",
    "                    'partition': relative_path,\n",
    "                    'size': size,\n",
    "                    'records': num_rows\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(partition_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ae56cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCatalog:\n",
    "    def __init__(self, catalog_path):\n",
    "        self.catalog_path = catalog_path\n",
    "        self.catalog = self._load_catalog()\n",
    "    \n",
    "    def _load_catalog(self):\n",
    "        \"\"\"Load existing catalog or create new one.\"\"\"\n",
    "        if os.path.exists(self.catalog_path):\n",
    "            with open(self.catalog_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {'datasets': {}}\n",
    "    \n",
    "    def _save_catalog(self):\n",
    "        \"\"\"Save catalog to disk.\"\"\"\n",
    "        os.makedirs(os.path.dirname(self.catalog_path), exist_ok=True)\n",
    "        with open(self.catalog_path, 'w') as f:\n",
    "            json.dump(self.catalog, f, indent=2)\n",
    "    \n",
    "    def register_dataset(self, dataset_name, file_path, metadata=None):\n",
    "        \"\"\"Register a new dataset with metadata.\"\"\"\n",
    "        # Get file stats\n",
    "        stats = os.stat(file_path)\n",
    "        \n",
    "        # Read sample of data for schema inference\n",
    "        df_sample = None\n",
    "        if file_path.endswith('.csv'):\n",
    "            df_sample = pd.read_csv(file_path, nrows=1000)\n",
    "        elif file_path.endswith('.parquet'):\n",
    "            df_sample = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Build metadata\n",
    "        dataset_metadata = {\n",
    "            'file_path': file_path,\n",
    "            'size_bytes': stats.st_size,\n",
    "            'created_at': datetime.fromtimestamp(stats.st_ctime).isoformat(),\n",
    "            'modified_at': datetime.fromtimestamp(stats.st_mtime).isoformat(),\n",
    "            'schema': {\n",
    "                'columns': list(df_sample.columns),\n",
    "                'dtypes': df_sample.dtypes.astype(str).to_dict()\n",
    "            },\n",
    "            'format': os.path.splitext(file_path)[1][1:],\n",
    "            'row_count': len(df_sample) if df_sample is not None else None,\n",
    "        }\n",
    "        \n",
    "        if metadata:\n",
    "            dataset_metadata.update(metadata)\n",
    "        \n",
    "        self.catalog['datasets'][dataset_name] = dataset_metadata\n",
    "        self._save_catalog()\n",
    "    \n",
    "    def get_dataset_info(self, dataset_name):\n",
    "        \"\"\"Get information about a registered dataset.\"\"\"\n",
    "        return self.catalog['datasets'].get(dataset_name)\n",
    "    \n",
    "    def list_datasets(self):\n",
    "        \"\"\"List all registered datasets.\"\"\"\n",
    "        return list(self.catalog['datasets'].keys())\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate a summary report of the data catalog.\"\"\"\n",
    "        report = {\n",
    "            'total_datasets': len(self.catalog['datasets']),\n",
    "            'total_size_bytes': sum(d['size_bytes'] for d in self.catalog['datasets'].values()),\n",
    "            'formats': {},\n",
    "            'created_dates': [],\n",
    "        }\n",
    "        \n",
    "        for dataset in self.catalog['datasets'].values():\n",
    "            fmt = dataset['format']\n",
    "            report['formats'][fmt] = report['formats'].get(fmt, 0) + 1\n",
    "            report['created_dates'].append(dataset['created_at'][:10])  # Just the date part\n",
    "        \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec0a554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing raw data files...\n",
      "\n",
      "Processing ingested_20250821_185705_customers.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 4.52x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 1.74x\n",
      "Creating partitioned structure...\n",
      "\n",
      "Partition Distribution:\n",
      "                                           partition   size  records\n",
      "0       contract=Month-to-month\\internet_service=DSL  11786      129\n",
      "1  contract=Month-to-month\\internet_service=Fiber...  12174      144\n",
      "2        contract=Month-to-month\\internet_service=No   9680       52\n",
      "3             contract=One year\\internet_service=DSL   9757       56\n",
      "4     contract=One year\\internet_service=Fiber optic   9701       50\n",
      "5              contract=One year\\internet_service=No   8603       15\n",
      "6             contract=Two year\\internet_service=DSL   8882       24\n",
      "7     contract=Two year\\internet_service=Fiber optic   8835       21\n",
      "8              contract=Two year\\internet_service=No   8436        9\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250821_185705_transactions.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 3.71x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 1.98x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250821_185841_customers.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 4.52x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 1.74x\n",
      "Creating partitioned structure...\n",
      "Parquet compression ratio: 1.98x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250821_185841_customers.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 4.52x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 1.74x\n",
      "Creating partitioned structure...\n",
      "\n",
      "Partition Distribution:\n",
      "                                           partition   size  records\n",
      "0       contract=Month-to-month\\internet_service=DSL  11786      129\n",
      "1  contract=Month-to-month\\internet_service=Fiber...  12174      144\n",
      "2        contract=Month-to-month\\internet_service=No   9680       52\n",
      "3             contract=One year\\internet_service=DSL   9757       56\n",
      "4     contract=One year\\internet_service=Fiber optic   9701       50\n",
      "5              contract=One year\\internet_service=No   8603       15\n",
      "6             contract=Two year\\internet_service=DSL   8882       24\n",
      "7     contract=Two year\\internet_service=Fiber optic   8835       21\n",
      "8              contract=Two year\\internet_service=No   8436        9\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250821_185841_transactions.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 3.71x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 1.98x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142601_telco_train.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 0.97x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 0.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142601_transactions.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 3.82x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 2.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142608_telco_train.csv:\n",
      "Compressing data...\n",
      "\n",
      "Partition Distribution:\n",
      "                                           partition   size  records\n",
      "0       contract=Month-to-month\\internet_service=DSL  11786      129\n",
      "1  contract=Month-to-month\\internet_service=Fiber...  12174      144\n",
      "2        contract=Month-to-month\\internet_service=No   9680       52\n",
      "3             contract=One year\\internet_service=DSL   9757       56\n",
      "4     contract=One year\\internet_service=Fiber optic   9701       50\n",
      "5              contract=One year\\internet_service=No   8603       15\n",
      "6             contract=Two year\\internet_service=DSL   8882       24\n",
      "7     contract=Two year\\internet_service=Fiber optic   8835       21\n",
      "8              contract=Two year\\internet_service=No   8436        9\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250821_185841_transactions.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 3.71x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 1.98x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142601_telco_train.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 0.97x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 0.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142601_transactions.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 3.82x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 2.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142608_telco_train.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 0.97x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 0.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142608_transactions.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 3.82x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 2.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142727_customers.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 4.96x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 3.03x\n",
      "Creating partitioned structure...\n",
      "Compression ratio: 0.97x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 0.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142608_transactions.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 3.82x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 2.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142727_customers.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 4.96x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 3.03x\n",
      "Creating partitioned structure...\n",
      "\n",
      "Partition Distribution:\n",
      "                                           partition   size  records\n",
      "0       contract=Month-to-month\\internet_service=DSL  42261     1310\n",
      "1  contract=Month-to-month\\internet_service=Fiber...  46505     1469\n",
      "2        contract=Month-to-month\\internet_service=No  21028      492\n",
      "3             contract=One year\\internet_service=DSL  20683      477\n",
      "4     contract=One year\\internet_service=Fiber optic  22691      546\n",
      "5              contract=One year\\internet_service=No  14142      219\n",
      "6             contract=Two year\\internet_service=DSL  12948      172\n",
      "7     contract=Two year\\internet_service=Fiber optic  14389      229\n",
      "8              contract=Two year\\internet_service=No  10734       86\n",
      "Registering in catalog...\n",
      "\n",
      "Partition Distribution:\n",
      "                                           partition   size  records\n",
      "0       contract=Month-to-month\\internet_service=DSL  42261     1310\n",
      "1  contract=Month-to-month\\internet_service=Fiber...  46505     1469\n",
      "2        contract=Month-to-month\\internet_service=No  21028      492\n",
      "3             contract=One year\\internet_service=DSL  20683      477\n",
      "4     contract=One year\\internet_service=Fiber optic  22691      546\n",
      "5              contract=One year\\internet_service=No  14142      219\n",
      "6             contract=Two year\\internet_service=DSL  12948      172\n",
      "7     contract=Two year\\internet_service=Fiber optic  14389      229\n",
      "8              contract=Two year\\internet_service=No  10734       86\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142727_telco_train.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 0.97x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 0.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142727_transactions.csv:\n",
      "Compressing data...\n",
      "\n",
      "Processing ingested_20250824_142727_telco_train.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 0.97x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 0.04x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_142727_transactions.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 3.90x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 2.57x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_143055_customers.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 4.96x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 3.03x\n",
      "Creating partitioned structure...\n",
      "\n",
      "Partition Distribution:\n",
      "                                           partition   size  records\n",
      "0       contract=Month-to-month\\internet_service=DSL  42261     1310\n",
      "1  contract=Month-to-month\\internet_service=Fiber...  46505     1469\n",
      "2        contract=Month-to-month\\internet_service=No  21028      492\n",
      "3             contract=One year\\internet_service=DSL  20683      477\n",
      "4     contract=One year\\internet_service=Fiber optic  22691      546\n",
      "5              contract=One year\\internet_service=No  14142      219\n",
      "6             contract=Two year\\internet_service=DSL  12948      172\n",
      "7     contract=Two year\\internet_service=Fiber optic  14389      229\n",
      "8              contract=Two year\\internet_service=No  10734       86\n",
      "Registering in catalog...\n",
      "Compression ratio: 3.90x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 2.57x\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_143055_customers.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 4.96x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 3.03x\n",
      "Creating partitioned structure...\n",
      "\n",
      "Partition Distribution:\n",
      "                                           partition   size  records\n",
      "0       contract=Month-to-month\\internet_service=DSL  42261     1310\n",
      "1  contract=Month-to-month\\internet_service=Fiber...  46505     1469\n",
      "2        contract=Month-to-month\\internet_service=No  21028      492\n",
      "3             contract=One year\\internet_service=DSL  20683      477\n",
      "4     contract=One year\\internet_service=Fiber optic  22691      546\n",
      "5              contract=One year\\internet_service=No  14142      219\n",
      "6             contract=Two year\\internet_service=DSL  12948      172\n",
      "7     contract=Two year\\internet_service=Fiber optic  14389      229\n",
      "8              contract=Two year\\internet_service=No  10734       86\n",
      "Registering in catalog...\n",
      "\n",
      "Processing ingested_20250824_143055_transactions.csv:\n",
      "Compressing data...\n",
      "\n",
      "Processing ingested_20250824_143055_transactions.csv:\n",
      "Compressing data...\n",
      "Compression ratio: 3.90x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 2.57x\n",
      "Registering in catalog...\n",
      "\n",
      "Data Catalog Report:\n",
      "--------------------------------------------------\n",
      "Total Datasets: 13\n",
      "Total Size: 2.74 MB\n",
      "\n",
      "File Formats:\n",
      "- csv: 13 files\n",
      "Compression ratio: 3.90x\n",
      "Converting to Parquet...\n",
      "Parquet compression ratio: 2.57x\n",
      "Registering in catalog...\n",
      "\n",
      "Data Catalog Report:\n",
      "--------------------------------------------------\n",
      "Total Datasets: 13\n",
      "Total Size: 2.74 MB\n",
      "\n",
      "File Formats:\n",
      "- csv: 13 files\n"
     ]
    }
   ],
   "source": [
    "# Initialize paths\n",
    "raw_root = os.path.join('..', 'data', 'raw')\n",
    "catalog_path = os.path.join(raw_root, 'catalog.json')\n",
    "compressed_root = os.path.join(raw_root, 'compressed')\n",
    "partitioned_root = os.path.join(raw_root, 'partitioned')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(compressed_root, exist_ok=True)\n",
    "os.makedirs(partitioned_root, exist_ok=True)\n",
    "\n",
    "# Initialize data catalog\n",
    "catalog = DataCatalog(catalog_path)\n",
    "\n",
    "# Process existing raw data\n",
    "print(\"Processing raw data files...\")\n",
    "for filename in os.listdir(raw_root):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "    \n",
    "    input_path = os.path.join(raw_root, filename)\n",
    "    print(f\"\\nProcessing {filename}:\")\n",
    "    \n",
    "    # 1. Compress data\n",
    "    print(\"Compressing data...\")\n",
    "    compressed_results = compress_csv(input_path, \n",
    "                                   os.path.join(compressed_root, f\"{filename}.gz\"))\n",
    "    print(f\"Compression ratio: {compressed_results['compression_ratio']:.2f}x\")\n",
    "    \n",
    "    # 2. Convert to Parquet\n",
    "    print(\"Converting to Parquet...\")\n",
    "    parquet_results = convert_to_parquet(input_path, \n",
    "                                       os.path.join(compressed_root, filename.replace('.csv', '.parquet')))\n",
    "    print(f\"Parquet compression ratio: {parquet_results['compression_ratio']:.2f}x\")\n",
    "    \n",
    "    # 3. Create partitioned structure for customer data\n",
    "    if 'customer' in filename.lower():\n",
    "        print(\"Creating partitioned structure...\")\n",
    "        df = pd.read_csv(input_path)\n",
    "        partition_path = os.path.join(partitioned_root, 'customers')\n",
    "        create_partitioned_structure(df, ['contract', 'internet_service'], partition_path)\n",
    "        \n",
    "        # Analyze partition distribution\n",
    "        partition_stats = analyze_partition_distribution(partition_path)\n",
    "        print(\"\\nPartition Distribution:\")\n",
    "        print(partition_stats)\n",
    "    \n",
    "    # 4. Register in catalog\n",
    "    print(\"Registering in catalog...\")\n",
    "    catalog.register_dataset(\n",
    "        dataset_name=filename,\n",
    "        file_path=input_path,\n",
    "        metadata={\n",
    "            'compressed_path': compressed_results['output_path'],\n",
    "            'parquet_path': parquet_results['output_path'] if 'parquet_path' in locals() else None,\n",
    "            'compression_stats': {\n",
    "                'original_size': compressed_results['original_size'],\n",
    "                'compressed_size': compressed_results['compressed_size'],\n",
    "                'compression_ratio': compressed_results['compression_ratio']\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Generate and display catalog report\n",
    "print(\"\\nData Catalog Report:\")\n",
    "print(\"-\" * 50)\n",
    "report = catalog.generate_report()\n",
    "print(f\"Total Datasets: {report['total_datasets']}\")\n",
    "print(f\"Total Size: {report['total_size_bytes'] / 1024 / 1024:.2f} MB\")\n",
    "print(\"\\nFile Formats:\")\n",
    "for fmt, count in report['formats'].items():\n",
    "    print(f\"- {fmt}: {count} files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
