{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b541c685",
   "metadata": {},
   "source": [
    "# 02 â€“ Data Ingestion\n",
    "\n",
    "This notebook demonstrates enhanced data ingestion capabilities including:\n",
    "- Automated data profiling\n",
    "- Data sampling for large datasets\n",
    "- Data quality checks during ingestion\n",
    "- Progress monitoring\n",
    "- Source data verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c41e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/shubham/Windows/Users/SBS05/Downloads/dm4ml_churn_pipeline_1/.venv4/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: `import pandas_profiling` is going to be deprecated by April 1st. Please use `import ydata_profiling` instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec919c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f3cbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 18:05:14 - ingest_csv - INFO - Ingested customers.csv with 5000 rows to ingested_20250824_180514_customers.csv\n",
      "2025-08-24 18:05:14 - ingest_csv - INFO - Ingested telco_train.csv with 0 rows to ingested_20250824_180514_telco_train.csv\n",
      "2025-08-24 18:05:14 - ingest_csv - INFO - Ingested telco_train.csv with 0 rows to ingested_20250824_180514_telco_train.csv\n",
      "2025-08-24 18:05:14 - ingest_csv - INFO - Ingested transactions.csv with 24911 rows to ingested_20250824_180514_transactions.csv\n",
      "2025-08-24 18:05:14 - ingest_api - INFO - Ingested web_logs.jsonl with 25099 events to ingested_20250824_180514_web_logs.jsonl\n",
      "2025-08-24 18:05:14 - ingest_csv - INFO - Ingested transactions.csv with 24911 rows to ingested_20250824_180514_transactions.csv\n",
      "2025-08-24 18:05:14 - ingest_api - INFO - Ingested web_logs.jsonl with 25099 events to ingested_20250824_180514_web_logs.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion complete.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from scripts.ingestion import ingest_csv, ingest_api\n",
    "\n",
    "# Define paths\n",
    "date_partition = \"20250821\"  # Using existing data folder instead of current date\n",
    "source_csv_dir = os.path.join('..', 'data', 'raw', 'source_csv', date_partition)\n",
    "source_api_file = os.path.join('..', 'data', 'raw', 'source_api', date_partition, 'web_logs.jsonl')\n",
    "raw_root = os.path.join('..', 'data', 'raw')\n",
    "\n",
    "# Ingest the CSV and API data\n",
    "ingest_csv(source_csv_dir, raw_root)\n",
    "ingest_api(source_api_file, raw_root)\n",
    "\n",
    "print('Ingestion complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d692c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_dataset(df, title):\n",
    "    \"\"\"Generate a detailed profile report for the dataset.\"\"\"\n",
    "    profile = ProfileReport(df, title=title, explorative=True)\n",
    "    return profile\n",
    "\n",
    "def sample_large_dataset(file_path, sample_size=10000, random_state=42):\n",
    "    \"\"\"Sample large datasets efficiently using chunking.\"\"\"\n",
    "    # Get total number of rows\n",
    "    total_rows = sum(1 for _ in open(file_path)) - 1  # subtract header\n",
    "    \n",
    "    if total_rows <= sample_size:\n",
    "        return pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate skip rows\n",
    "    skip_rate = total_rows // sample_size\n",
    "    skip_indices = set(range(1, total_rows + 1))  # keep header (0)\n",
    "    skip_indices -= set(range(1, total_rows + 1, skip_rate))  # keep every nth row\n",
    "    skip_indices = sorted(list(skip_indices))\n",
    "    \n",
    "    return pd.read_csv(file_path, skiprows=skip_indices)\n",
    "\n",
    "def verify_data_schema(df, expected_schema):\n",
    "    \"\"\"Verify that the dataframe matches expected schema.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check columns\n",
    "    missing_cols = set(expected_schema['columns']) - set(df.columns)\n",
    "    extra_cols = set(df.columns) - set(expected_schema['columns'])\n",
    "    \n",
    "    if missing_cols:\n",
    "        issues.append(f\"Missing columns: {missing_cols}\")\n",
    "    if extra_cols:\n",
    "        issues.append(f\"Extra columns: {extra_cols}\")\n",
    "    \n",
    "    # Check data types\n",
    "    for col, dtype in expected_schema['dtypes'].items():\n",
    "        if col in df.columns:\n",
    "            if not pd.api.types.is_dtype_equal(df[col].dtype, dtype):\n",
    "                issues.append(f\"Column {col} has dtype {df[col].dtype}, expected {dtype}\")\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def plot_data_distribution(df):\n",
    "    \"\"\"Plot distribution of numerical and categorical columns.\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    # Plot numerical distributions\n",
    "    if len(numerical_cols) > 0:\n",
    "        fig, axes = plt.subplots(len(numerical_cols), 1, figsize=(12, 4*len(numerical_cols)))\n",
    "        if len(numerical_cols) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, col in zip(axes, numerical_cols):\n",
    "            sns.histplot(data=df, x=col, ax=ax)\n",
    "            ax.set_title(f'Distribution of {col}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot categorical distributions\n",
    "    if len(categorical_cols) > 0:\n",
    "        fig, axes = plt.subplots(len(categorical_cols), 1, figsize=(12, 4*len(categorical_cols)))\n",
    "        if len(categorical_cols) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for ax, col in zip(axes, categorical_cols):\n",
    "            value_counts = df[col].value_counts()\n",
    "            if value_counts.empty:\n",
    "                ax.set_title(f'Distribution of {col} (no data)')\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            value_counts.plot(kind='bar', ax=ax)\n",
    "            ax.set_title(f'Distribution of {col}')\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f857692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expected schemas\n",
    "schemas = {\n",
    "    'customers': {\n",
    "        'columns': ['customer_id', 'gender', 'senior_citizen', 'partner', 'dependents',\n",
    "                   'tenure_months', 'monthly_charges', 'total_charges', 'contract',\n",
    "                   'internet_service', 'phone_service', 'churn'],\n",
    "        'dtypes': {\n",
    "            'customer_id': 'object',\n",
    "            'gender': 'object',\n",
    "            'senior_citizen': 'int64',\n",
    "            'partner': 'object',\n",
    "            'dependents': 'object',\n",
    "            'tenure_months': 'int64',\n",
    "            'monthly_charges': 'float64',\n",
    "            'total_charges': 'float64',\n",
    "            'contract': 'object',\n",
    "            'internet_service': 'object',\n",
    "            'phone_service': 'object',\n",
    "            'churn': 'object'\n",
    "        }\n",
    "    },\n",
    "    'transactions': {\n",
    "        'columns': ['transaction_id', 'customer_id', 'transaction_date', 'amount'],\n",
    "        'dtypes': {\n",
    "            'transaction_id': 'object',\n",
    "            'customer_id': 'object',\n",
    "            'transaction_date': 'object',\n",
    "            'amount': 'float64'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f3af66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing customers.csv:\n",
      "Loading and sampling data...\n",
      "Verifying schema...\n",
      "Schema verification passed!\n",
      "Generating profile report...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f643550bdfe64e419419f3519132b5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3da4dee81464d33817bcd9512a2c24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a856666db2c74e528d9435ddc0bcaebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b484655307614701bebfee4434376f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile report saved to ../data/raw/customers_profile.html\n",
      "Plotting distributions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/shubham/Windows/Users/SBS05/Downloads/dm4ml_churn_pipeline_1/.venv4/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing telco_train.csv:\n",
      "Loading and sampling data...\n",
      "Skipped profiling for telco_train.csv: DataFrame is empty.\n",
      "Plotting distributions...\n",
      "\n",
      "Processing transactions.csv:\n",
      "Loading and sampling data...\n",
      "Verifying schema...\n",
      "Schema verification passed!\n",
      "Generating profile report...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/shubham/Windows/Users/SBS05/Downloads/dm4ml_churn_pipeline_1/.venv4/lib/python3.7/site-packages/ipykernel_launcher.py:76: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717c714a89a04028870b8e3e44392116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06eb529780024261bc687093c045f352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b218b37ebe449fa1e70d4fca972bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bb27ced514438abe243e6aef723d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile report saved to ../data/raw/transactions_profile.html\n",
      "Plotting distributions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/shubham/Windows/Users/SBS05/Downloads/dm4ml_churn_pipeline_1/.venv4/lib/python3.7/site-packages/ipykernel_launcher.py:58: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "/media/shubham/Windows/Users/SBS05/Downloads/dm4ml_churn_pipeline_1/.venv4/lib/python3.7/site-packages/ipykernel_launcher.py:76: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "2025-08-24 18:05:59 - ingest_csv - INFO - Ingested customers.csv with 5000 rows to ingested_20250824_180559_customers.csv\n",
      "2025-08-24 18:05:59 - ingest_csv - INFO - Ingested telco_train.csv with 0 rows to ingested_20250824_180559_telco_train.csv\n",
      "/media/shubham/Windows/Users/SBS05/Downloads/dm4ml_churn_pipeline_1/.venv4/lib/python3.7/site-packages/ipykernel_launcher.py:76: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "2025-08-24 18:05:59 - ingest_csv - INFO - Ingested customers.csv with 5000 rows to ingested_20250824_180559_customers.csv\n",
      "2025-08-24 18:05:59 - ingest_csv - INFO - Ingested telco_train.csv with 0 rows to ingested_20250824_180559_telco_train.csv\n",
      "2025-08-24 18:05:59 - ingest_csv - INFO - Ingested transactions.csv with 24911 rows to ingested_20250824_180559_transactions.csv\n",
      "2025-08-24 18:05:59 - ingest_api - INFO - Ingested web_logs.jsonl with 25099 events to ingested_20250824_180559_web_logs.jsonl\n",
      "2025-08-24 18:05:59 - ingest_csv - INFO - Ingested transactions.csv with 24911 rows to ingested_20250824_180559_transactions.csv\n",
      "2025-08-24 18:05:59 - ingest_api - INFO - Ingested web_logs.jsonl with 25099 events to ingested_20250824_180559_web_logs.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingesting CSV data...\n",
      "\n",
      "Ingesting API data...\n",
      "\n",
      "Ingestion complete with enhanced profiling and validation!\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "date_partition = \"20250821\"  # Using existing data folder\n",
    "source_csv_dir = os.path.join('..', 'data', 'raw', 'source_csv', date_partition)\n",
    "source_api_file = os.path.join('..', 'data', 'raw', 'source_api', date_partition, 'web_logs.jsonl')\n",
    "raw_root = os.path.join('..', 'data', 'raw')\n",
    "\n",
    "# Process CSV files with profiling and validation\n",
    "for filename in os.listdir(source_csv_dir):\n",
    "    if not filename.endswith('.csv'):\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nProcessing {filename}:\")\n",
    "    file_path = os.path.join(source_csv_dir, filename)\n",
    "    \n",
    "    # Sample data if file is large\n",
    "    print(\"Loading and sampling data...\")\n",
    "    df = sample_large_dataset(file_path)\n",
    "    \n",
    "    # Verify schema\n",
    "    dataset_name = filename.replace('.csv', '')\n",
    "    if dataset_name in schemas:\n",
    "        print(\"Verifying schema...\")\n",
    "        issues = verify_data_schema(df, schemas[dataset_name])\n",
    "        if issues:\n",
    "            print(\"Schema issues found:\")\n",
    "            for issue in issues:\n",
    "                print(f\"- {issue}\")\n",
    "        else:\n",
    "            print(\"Schema verification passed!\")\n",
    "    \n",
    "    # Generate and save profile report only if DataFrame is not empty\n",
    "    if not df.empty:\n",
    "        print(\"Generating profile report...\")\n",
    "        profile = profile_dataset(df, f\"Data Profile - {filename}\")\n",
    "        profile_path = os.path.join(raw_root, f\"{dataset_name}_profile.html\")\n",
    "        profile.to_file(profile_path)\n",
    "        print(f\"Profile report saved to {profile_path}\")\n",
    "    else:\n",
    "        print(f\"Skipped profiling for {filename}: DataFrame is empty.\")\n",
    "    \n",
    "    # Plot distributions\n",
    "    print(\"Plotting distributions...\")\n",
    "    plot_data_distribution(df)\n",
    "    \n",
    "# Ingest the data using existing functions\n",
    "from scripts.ingestion import ingest_csv, ingest_api\n",
    "\n",
    "print(\"\\nIngesting CSV data...\")\n",
    "ingest_csv(source_csv_dir, raw_root)\n",
    "\n",
    "print(\"\\nIngesting API data...\")\n",
    "ingest_api(source_api_file, raw_root)\n",
    "\n",
    "print('\\nIngestion complete with enhanced profiling and validation!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
